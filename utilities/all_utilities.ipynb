{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ChatterChop\\.venv\\lib\\site-packages\\whisper\\timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n",
      "d:\\ChatterChop\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import os\n",
    "\n",
    "from evaluate import load\n",
    "import audiosegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audio transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcription(path_to_audio_file, path_to_transcription_file, model_name='small', language='pl'):\n",
    "\n",
    "    model = whisper.load_model(model_name)\n",
    "    result = model.transcribe(path_to_audio_file, language=language)\n",
    "\n",
    "    with open(path_to_transcription_file, \"a\", encoding=\"utf-8\") as file_txt:\n",
    "        file_txt.write(f\"{result['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_relative_path = os.path.join(\"..\", \"test_data\", \"test_audio_pl.wav\")\n",
    "transcription_relative_path = os.path.join(\"..\", \"test_data\", \"test_transcription.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription(audio_relative_path, transcription_relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcription metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = os.path.join(\"..\", \"test_data\", \"test_transcription_ground_truth.txt\")\n",
    "transcription_to_test = os.path.join(\"..\", \"test_data\", \"test_transcription.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_into_list(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text = text.strip()\n",
    "            text = ''.join(char.lower() if char.isalnum() or char.isspace() else ' ' for char in text)\n",
    "            one_long_sentence = ' '.join(text.split())\n",
    "            sentence_list = [one_long_sentence]\n",
    "            return sentence_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_transcription_to_test = txt_into_list(transcription_to_test)\n",
    "list_ground_truth = txt_into_list(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['warszawa jest miastem pełnym sprzeczności przez wielu nazywanym niezniszczalnym miasto to wzbudza zachwyt prowokuje i inspiruje panującą w nim atmosferę o magicznym duchu nieistniejących już miejsc tworząc unikalne zestawienie zabytkowych budynków z nowoczesną miejską architekturą a przykładem niezwykłej niezłomności stolicy jest jej stare miasto warszawa jest miastem pełnym sprzeczności przez wielu nazywanym niezniszczalnym']\n",
      "['warszawa jest miastem pełnym sprzeczności przez wielu nazywanym niezniszczalnym miasto to wzbudza zachwyt prowokuje i inspiruje panującą w nim atmosferą o magicznym duchu nieistniejących już miejsc tworząc unikalne zestawienie zabytkowych budynków z nowoczesną miejską architekturą a przykładem niezwykłej niezłomności stolicy jest jej stare miasto warszawa jest miastem pełnym sprzeczności przez wielu nazywanym niezniszczalnym']\n"
     ]
    }
   ],
   "source": [
    "print(list_transcription_to_test)\n",
    "print(list_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018867924528301886\n"
     ]
    }
   ],
   "source": [
    "wer = load('wer')\n",
    "wer_score = wer.compute(predictions=list_transcription_to_test, references=list_ground_truth)\n",
    "print(wer_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002336448598130841\n"
     ]
    }
   ],
   "source": [
    "cer = load(\"cer\")\n",
    "cer_score = cer.compute(predictions=list_transcription_to_test, references=list_ground_truth)\n",
    "print(cer_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_words_to_file(sentence, output_file_path):\n",
    "    try:\n",
    "        if len(sentence) == 1:\n",
    "            words = sentence[0].split()\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                for word in words:\n",
    "                    output_file.write(word + '\\n')\n",
    "        else:\n",
    "            print(\"Invalid input. The 'sentence' parameter should be a list containing one long string.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_words = os.path.join(\"..\", \"test_data\", \"word_by_word.txt\")\n",
    "write_words_to_file(list_ground_truth, splitted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform VAD and split the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio_path, transcription):\n",
    "    if not transcription:\n",
    "        return\n",
    "\n",
    "    audio = audiosegment.from_file(audio_path)\n",
    "    word_segments = []\n",
    "\n",
    "    for result in transcription.get(\"alternative\", []):\n",
    "        transcript = result.get(\"transcript\", \"\")\n",
    "        if transcript:\n",
    "            for word_info in result.get(\"words\", []):\n",
    "                start_time = float(word_info.get(\"start\", 0))\n",
    "                end_time = float(word_info.get(\"end\", 0))\n",
    "                word_audio = audio[start_time * 1000:end_time * 1000]\n",
    "                word_segments.append(word_audio)\n",
    "\n",
    "    return word_segments\n",
    "\n",
    "def vad(audio_path):\n",
    "    audio = audiosegment.from_file(audio_path)\n",
    "    speech_segments = []\n",
    "\n",
    "    silence = audiosegment.empty()\n",
    "    current_segment = []\n",
    "\n",
    "    for segment in audio:\n",
    "        if segment.dBFS > silence.dBFS:\n",
    "            current_segment.append(segment)\n",
    "        else:\n",
    "            if current_segment:\n",
    "                speech_segments.append(audiosegment.from_mono_audiosegments(*current_segment))\n",
    "            current_segment = []\n",
    "\n",
    "    return speech_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1: play some Justin\n",
      "Word 2: news\n",
      "Word 3: play insper\n",
      "Word 4: who is Buddha Zack\n",
      "Word 5: show me calendar\n",
      "Word 6: no\n",
      "Word 7: what's the weather music\n",
      "Word 8: play SDA\n",
      "Word 9: stop\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "audio_path = os.path.join(\"..\", \"test_data\", \"test_audio_pl.wav\")\n",
    "audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "# Initialize the speech recognition engine\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Split the audio into segments based on speech detection\n",
    "def split_audio_segments(audio):\n",
    "    audio_duration = len(audio)\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    segments = []\n",
    "\n",
    "    while start_time < audio_duration:\n",
    "        # Set the end time for the next segment\n",
    "        end_time = start_time + 1000  # Adjust this value as needed\n",
    "\n",
    "        if end_time > audio_duration:\n",
    "            end_time = audio_duration\n",
    "\n",
    "        # Extract the segment\n",
    "        segment = audio[start_time:end_time]\n",
    "\n",
    "        # Recognize speech in the segment\n",
    "        with sr.AudioFile(segment.export(format=\"wav\")) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            try:\n",
    "                # Use Google Web Speech API for speech recognition\n",
    "                word = recognizer.recognize_google(audio_data)\n",
    "                segments.append(word)\n",
    "            except sr.UnknownValueError:\n",
    "                pass\n",
    "\n",
    "        # Move to the next segment\n",
    "        start_time = end_time\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Split the audio into word segments\n",
    "word_segments = split_audio_segments(audio)\n",
    "\n",
    "# Print the word segments\n",
    "for idx, word in enumerate(word_segments):\n",
    "    print(f\"Word {idx + 1}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import soundfile as sf\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "\n",
    "audio_path = os.path.join(\"..\", \"test_data\", \"test_audio_pl.wav\")\n",
    "output_path = os.path.join(\"..\", \"test_data\", \"test_split\")\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Define frame parameters (adjust as needed)\n",
    "frame_length = 512  # Length of each frame in samples\n",
    "hop_length = 256     # Hop size (overlap) between frames in samples\n",
    "\n",
    "# Calculate the short-term energy of each frame\n",
    "frame_y = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n",
    "energy = np.sum(np.abs(frame_y), axis=0)\n",
    "for i, ener in enumerate(energy):\n",
    "    print(ener)\n",
    "# Set a threshold to distinguish between speech and silence\n",
    "energy_threshold = 2.25  # Adjust as needed\n",
    "\n",
    "# Initialize variables for word segmentation\n",
    "word_segments = []\n",
    "start_frame = None\n",
    "\n",
    "# Iterate through frames and perform word segmentation\n",
    "for frame_idx, frame_energy in enumerate(energy):\n",
    "    if frame_energy > energy_threshold and start_frame is None:\n",
    "        # Start of a word\n",
    "        start_frame = frame_idx\n",
    "        print(frame_energy)\n",
    "    elif frame_energy <= energy_threshold and start_frame is not None:\n",
    "        print(frame_energy)\n",
    "        end_frame = frame_idx\n",
    "        word_segments.append((start_frame, end_frame))\n",
    "        start_frame = None\n",
    "\n",
    "for segment_idx, (start_frame, end_frame) in enumerate(word_segments):\n",
    "    start_sample = start_frame * hop_length\n",
    "    end_sample = end_frame * hop_length\n",
    "    word_audio = y[start_sample:end_sample]\n",
    "\n",
    "    sf.write(f'{os.path.join(output_path, str(segment_idx))}.wav', word_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "audio_path = os.path.join(\"..\", \"test_data\", \"test_audio_pl.wav\")\n",
    "\n",
    "audio_file = \"your_audio_file.wav\"\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Create the Mel spectrogram\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "# Convert power spectrogram to dB scale\n",
    "mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', sr=sr, cmap='viridis')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mel Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
